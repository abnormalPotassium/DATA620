{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kq75V0AUSB7Q"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abnormalPotassium/DATA620/blob/main/Project%203/Project%203%20Attempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3: Names Corpus Classifiers\n",
        "\n",
        "By: Al Haque, Taha Ahmad"
      ],
      "metadata": {
        "id": "CcZGeYKPCeSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "## Goal"
      ],
      "metadata": {
        "id": "1GeSQDPeDB6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project's goal is to:\n",
        "-  Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can.\n",
        "  -  Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set.\n",
        "  -  Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress.\n",
        "  -  Once you are satisfied with your classifier, check its final performance on the test set.\n",
        "  -  Describe how the performance on the test set compares to the performance on the dev-test set and if the divergence is expected.\n"
      ],
      "metadata": {
        "id": "-T22ZdovCj0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Package Installation"
      ],
      "metadata": {
        "id": "Kq75V0AUSB7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any packages that need to be installed for working on the classifiers can be added in the code block below. The very initial package assumption is that we'll simply need nltk and possibly pandas."
      ],
      "metadata": {
        "id": "ITz6TsrANhO6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10LeWLa5KBGR"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "## Dataset Loading"
      ],
      "metadata": {
        "id": "BkVXljx_Dwut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Our data](http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/0.html) is a set of names collected by Mark Kantrowitz and Bill Ross in 1994 where the names are separated in files by gender. There are 7944 observations in total with 5001 female names and 2943 males that are sorted alphabetically.\n",
        "\n",
        "The nltk package allows for directly downloading and accessing this dataset which we do below. Note that dataset loading process is largely identical to sample code provided in the [nltk book](https://www.nltk.org/book/ch06.html) by Steven Bird, Ewan Klein, and Edward Loper."
      ],
      "metadata": {
        "id": "t55GNXkUHSEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('names')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuVmVUVV9xGQ",
        "outputId": "ff4f6a7c-a8f3-4a79-a18d-4514ee399ad6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the dataset that consists of two separate files for male and female, we create a combined list of lists which has each name paired with its gender. Since, these are initially sorted alphabetically we used the random package's ability to shuffle lists in place to allow us to split the data randomly. Note that a seed is set here to encourage reproducibility."
      ],
      "metadata": {
        "id": "YRv-d1JcBb-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import names\n",
        "import random\n",
        "\n",
        "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
        "\n",
        "random.seed(1337)\n",
        "random.shuffle(labeled_names)"
      ],
      "metadata": {
        "id": "80VwrO_E9isZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the `labeled_names` list into a training set with 6944 observations, a dev-test set with 500 observations, and a test set with 500 observations. The training set will be used for training the models, the dev-test set will be used for initially testing the trained models while further developing them, and the test set will be used for the final performance test.\n",
        "\n"
      ],
      "metadata": {
        "id": "mY_t9lJEDIQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_names = labeled_names[1000:]\n",
        "devtest_names = labeled_names[500:1000]\n",
        "test_names = labeled_names[:500]"
      ],
      "metadata": {
        "id": "vZkUCpyh-WKL"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Model Building"
      ],
      "metadata": {
        "id": "0dVuoU2SsKw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our dataset split we can now focus on building a model to classify gender from names. We will focus on three different types of classifiers and optimizing these three classifiers to get the best predictive performance out of them."
      ],
      "metadata": {
        "id": "MjB1GT36r84c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Classifier 1: Naive Bayes"
      ],
      "metadata": {
        "id": "hraJFM3V8lt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Book Base"
      ],
      "metadata": {
        "id": "aPuW4VnNHflY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin the base of our naive Bayes classifier by looking at how the ntlk book tackles it. We have three different ways to extract features from our dataset:\n",
        "\n",
        "1.   A very simple approach that uses the last letter of the name as the singular feature.\n",
        "2.   A complex approach that predicts based on the first letter of the name, the last letter of the name, and two features for every single letter in the alphabet based on if a letter is present in the name and how many of the letter are present.\n",
        "3.   A simple approach that uses the last two letters of the name as features.\n",
        "\n"
      ],
      "metadata": {
        "id": "GcNV9WceEyHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features1(word):\n",
        "  return {'last_letter': word[-1].lower()}\n",
        "\n",
        "def gender_features2(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "def gender_features3(word):\n",
        "  return {'suffix1': word[-1:].lower(),\n",
        "          'suffix2': word[-2:].lower()}\n",
        "\n",
        "def gender_features(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    features[\"suffix1\"] = name[-1:].lower()\n",
        "    features[\"suffix2\"] = name[-2:].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features"
      ],
      "metadata": {
        "id": "aS4oHiXB_yuv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we check the accuracy of each of these feature sets trained to a naive Bayes model we see that the more features we have the greater the accuracy. However, the book showcases that our complex featureset actually ends up overfitting the test set. Thus, our starting point for improvement should be adding and modifying features to the suffix based classifier."
      ],
      "metadata": {
        "id": "P5Ba0imxMcQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set1 = [(gender_features1(n), gender) for (n, gender) in train_names]\n",
        "classifier1 = nltk.NaiveBayesClassifier.train(train_set1)\n",
        "\n",
        "train_set2 = [(gender_features2(n), gender) for (n, gender) in train_names]\n",
        "classifier2 = nltk.NaiveBayesClassifier.train(train_set2)\n",
        "\n",
        "train_set3 = [(gender_features3(n), gender) for (n, gender) in train_names]\n",
        "classifier3 = nltk.NaiveBayesClassifier.train(train_set3)\n",
        "\n",
        "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our last letter based classifier is {nltk.classify.accuracy(classifier1, devtest_set)*100}%\n",
        "The gender prediction accuracy for our large featureset classifier is {nltk.classify.accuracy(classifier2, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix based classifier is {nltk.classify.accuracy(classifier3, devtest_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXRclAdI_4Vo",
        "outputId": "11b4a70b-1a2d-4d98-8ec7-469d81dc1312"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our last letter based classifier is 77.2%\n",
            "The gender prediction accuracy for our large featureset classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix based classifier is 78.2%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building Upon The Base"
      ],
      "metadata": {
        "id": "xKRbXLngK9E8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we begin working on improving our existing naive Bayes model. We can take some insight into what features are most important by using the informative features function. Perhaps unsurprisingly, we see that using the last letter is the most important for our first two classifiers, while the suffixes provide much more information with higher ratios of determination."
      ],
      "metadata": {
        "id": "ghJStI8QNkKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier1.show_most_informative_features(5)\n",
        "print(\"\")\n",
        "classifier2.show_most_informative_features(5)\n",
        "print(\"\")\n",
        "classifier3.show_most_informative_features(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF5AIxbNs1gP",
        "outputId": "16eaded4-a7e2-47e1-a71f-b97547cd3dc2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             last_letter = 'k'              male : female =     39.4 : 1.0\n",
            "             last_letter = 'a'            female : male   =     34.7 : 1.0\n",
            "             last_letter = 'f'              male : female =     12.5 : 1.0\n",
            "             last_letter = 'p'              male : female =     11.8 : 1.0\n",
            "             last_letter = 'v'              male : female =      9.8 : 1.0\n",
            "\n",
            "Most Informative Features\n",
            "             last_letter = 'k'              male : female =     39.4 : 1.0\n",
            "             last_letter = 'a'            female : male   =     34.7 : 1.0\n",
            "             last_letter = 'f'              male : female =     12.5 : 1.0\n",
            "             last_letter = 'p'              male : female =     11.8 : 1.0\n",
            "             last_letter = 'v'              male : female =      9.8 : 1.0\n",
            "\n",
            "Most Informative Features\n",
            "                 suffix2 = 'na'           female : male   =    157.3 : 1.0\n",
            "                 suffix2 = 'la'           female : male   =     71.9 : 1.0\n",
            "                 suffix1 = 'k'              male : female =     39.4 : 1.0\n",
            "                 suffix2 = 'ia'           female : male   =     36.9 : 1.0\n",
            "                 suffix2 = 'us'             male : female =     35.4 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first modification will be adding the length of the name to determine if it is a useful indicator for classification."
      ],
      "metadata": {
        "id": "CtQ1ijdBuPwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features4(word):\n",
        "  return {'suffix1': word[-1:].lower(),\n",
        "          'suffix2': word[-2:].lower(),\n",
        "          'length': len(word)}\n",
        "\n",
        "def gender_features(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    features[\"suffix1\"] = name[-1:].lower()\n",
        "    features[\"suffix2\"] = name[-2:].lower()\n",
        "    features[\"length\"] = len(name)\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "train_set4 = [(gender_features4(n), gender) for (n, gender) in train_names]\n",
        "classifier4 = nltk.NaiveBayesClassifier.train(train_set4)\n",
        "\n",
        "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our suffix and length classifier is {nltk.classify.accuracy(classifier4, devtest_set)*100}%\n",
        "The gender prediction accuracy for our large featureset classifier is {nltk.classify.accuracy(classifier2, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix based classifier is {nltk.classify.accuracy(classifier3, devtest_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLyLi6HMud9W",
        "outputId": "0926f866-4048-49b8-e631-94412a0be29b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our suffix and length classifier is 77.8%\n",
            "The gender prediction accuracy for our large featureset classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix based classifier is 78.2%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps unsurprisingly, the length of a name does not matter much for determining the gender of the name. We have an accuracy measure that ends up worse than our purely suffix based classifier which is extremely interesting. As generally more features in a model lead to higher accuracy. What's likely happening here is since naive bayes models utilize each feature individually to contribute to the classification, our length feature is working against our suffix features.\n",
        "\n",
        "Checking this we can also see that length does not appear anywhere in the top 10 and in fact ends up in the 100s when checked further."
      ],
      "metadata": {
        "id": "DiNwTkdDvMIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier4.show_most_informative_features(10)\n",
        "# Ran separately: classifier4.show_most_informative_features(200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeA1SSLQvFbQ",
        "outputId": "17292ef2-4373-410a-abd7-610118869b43"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                 suffix2 = 'na'           female : male   =    157.3 : 1.0\n",
            "                 suffix2 = 'la'           female : male   =     71.9 : 1.0\n",
            "                 suffix1 = 'k'              male : female =     39.4 : 1.0\n",
            "                 suffix2 = 'ia'           female : male   =     36.9 : 1.0\n",
            "                 suffix2 = 'us'             male : female =     35.4 : 1.0\n",
            "                 suffix1 = 'a'            female : male   =     34.7 : 1.0\n",
            "                 suffix2 = 'sa'           female : male   =     34.0 : 1.0\n",
            "                 suffix2 = 'ra'           female : male   =     34.0 : 1.0\n",
            "                 suffix2 = 'rt'             male : female =     32.1 : 1.0\n",
            "                 suffix2 = 'ch'             male : female =     24.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon researching further on what makes a female name and male name different, it seems that the beginning of names is another indicator of gender. A softer beginning prefix indicates that the name is more likely to be female while a harder prefix indicates that the names is more likely to be male. To add this to our classifier we can have prefix features added to our suffix classifier."
      ],
      "metadata": {
        "id": "jI8mEbdDvCcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features5(word):\n",
        "  return {'suffix1': word[-1:].lower(),\n",
        "          'suffix2': word[-2:].lower(),\n",
        "          'prefix1': word[0].lower(),\n",
        "          'prefix2': word[0:2].lower()}\n",
        "\n",
        "def gender_features(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    features[\"suffix1\"] = name[-1:].lower()\n",
        "    features[\"suffix2\"] = name[-2:].lower()\n",
        "    features[\"prefix1\"] = name[0].lower()\n",
        "    features[\"prefix2\"] = name[0:2].lower()\n",
        "    features[\"length\"] = len(name)\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "\n",
        "train_set5 = [(gender_features5(n), gender) for (n, gender) in train_names]\n",
        "classifier5 = nltk.NaiveBayesClassifier.train(train_set5)\n",
        "\n",
        "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our suffix and prefix classifier is {nltk.classify.accuracy(classifier5, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix and length classifier is {nltk.classify.accuracy(classifier4, devtest_set)*100}%\n",
        "The gender prediction accuracy for our large featureset classifier is {nltk.classify.accuracy(classifier2, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix based classifier is {nltk.classify.accuracy(classifier3, devtest_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Uxxa73xfYp",
        "outputId": "4225a996-1cbd-4b17-eda9-483bba47aadc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our suffix and prefix classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix and length classifier is 77.8%\n",
            "The gender prediction accuracy for our large featureset classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix based classifier is 78.2%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We manage to increase classifier accuracy by 2% through adding our prefixes. Which makes the accuracy of this classifier match the large feature set classifier, but with less features. Providing this model isn't overfitting, this might be the final model we want to use. Of course, this all depends on testing against the actual test set of names.\n",
        "\n",
        "Still, suffixes can be shown to be more important in determining the gender of a name with our naive-bayes classifier below:"
      ],
      "metadata": {
        "id": "N-Nb96Ezy110"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier5.show_most_informative_features(10)\n",
        "# Ran separately: classifier5.show_most_informative_features(200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrUc4jJiyF4j",
        "outputId": "2b7def92-92fc-4abe-8e67-588d8e2fe6bd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "                 suffix2 = 'na'           female : male   =    157.3 : 1.0\n",
            "                 suffix2 = 'la'           female : male   =     71.9 : 1.0\n",
            "                 suffix1 = 'k'              male : female =     39.4 : 1.0\n",
            "                 suffix2 = 'ia'           female : male   =     36.9 : 1.0\n",
            "                 suffix2 = 'us'             male : female =     35.4 : 1.0\n",
            "                 suffix1 = 'a'            female : male   =     34.7 : 1.0\n",
            "                 suffix2 = 'sa'           female : male   =     34.0 : 1.0\n",
            "                 suffix2 = 'ra'           female : male   =     34.0 : 1.0\n",
            "                 suffix2 = 'rt'             male : female =     32.1 : 1.0\n",
            "                 suffix2 = 'ch'             male : female =     24.8 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A final modification to this classifier we want to make is reducing features to just the two letter suffix and prefix to attempt to reduce any potential overfitting."
      ],
      "metadata": {
        "id": "fnFfLxOkznKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gender_features6(word):\n",
        "  return {'suffix2': word[-2:].lower(),\n",
        "          'prefix2': word[0:2].lower()}\n",
        "\n",
        "train_set6 = [(gender_features6(n), gender) for (n, gender) in train_names]\n",
        "\n",
        "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
        "\n",
        "classifier6 = nltk.NaiveBayesClassifier.train(train_set6)\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our cut down suffix and prefix classifier is {nltk.classify.accuracy(classifier6, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix and prefix classifier is {nltk.classify.accuracy(classifier5, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix and length classifier is {nltk.classify.accuracy(classifier4, devtest_set)*100}%\n",
        "The gender prediction accuracy for our large featureset classifier is {nltk.classify.accuracy(classifier2, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix based classifier is {nltk.classify.accuracy(classifier3, devtest_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVnQR_etz4ew",
        "outputId": "d5d57a31-1cd3-4b7c-d533-cc2cc8610f0e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our cut down suffix and prefix classifier is 82.19999999999999%\n",
            "The gender prediction accuracy for our suffix and prefix classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix and length classifier is 77.8%\n",
            "The gender prediction accuracy for our large featureset classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix based classifier is 78.2%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are able to increase our accuracy and hopefully reduce overfitting by taking away any influence singular character suffix and prefixes had on our names."
      ],
      "metadata": {
        "id": "wh6EAnna0NiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Classifier 2: Maximum Entropy Model"
      ],
      "metadata": {
        "id": "t1HnWMs68tOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The maximum entropy model uses maximum likelihood estimation to have each feature of the model contribute to the classification in a combined manner. While a naive bayes model has each feature contribute in an individual manner. Let's see how well a few of our feature sets fare with this model. Ideally, we would have larger feature sets become more accurate within a maximum entropy model due to features harmonizing together rather than tugging in individual directions.\n"
      ],
      "metadata": {
        "id": "1jjoCz_hK1KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
        "\n",
        "maxent_classifier6 = nltk.MaxentClassifier.train(train_set6, trace=0)\n",
        "maxent_classifier3 = nltk.MaxentClassifier.train(train_set3, trace=0)\n",
        "maxent_classifier5 = nltk.MaxentClassifier.train(train_set5, trace=0)\n",
        "maxent_classifier2 = nltk.MaxentClassifier.train(train_set2, trace=0)\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our cut down suffix and prefix classifier is {nltk.classify.accuracy(maxent_classifier6, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix and prefix classifier is {nltk.classify.accuracy(maxent_classifier5, devtest_set)*100}%\n",
        "The gender prediction accuracy for our large featureset classifier is {nltk.classify.accuracy(maxent_classifier2, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix based classifier is {nltk.classify.accuracy(maxent_classifier3, devtest_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IxOTV44-zCF",
        "outputId": "f65057a8-e7fe-49b5-c5d8-c5b6e8d603f6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our cut down suffix and prefix classifier is 81.39999999999999%\n",
            "The gender prediction accuracy for our suffix and prefix classifier is 81.6%\n",
            "The gender prediction accuracy for our large featureset classifier is 82.19999999999999%\n",
            "The gender prediction accuracy for our suffix based classifier is 77.0%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how a maximum entropy model utilizes bigger feature sizes better than a naive bayes model with our large featureset features and suffix and prefix features increasing in accuracy while our smaller models decrease in accuracy.\n",
        "\n",
        "Surpisingly our cut down suffix and prefix model is simpler but still has a better accuracy than every classifier except for the large featureset max entropy model. So, we keep it as a candidate for our final model."
      ],
      "metadata": {
        "id": "GdtQhcRS-3Gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Classifier 3: Decision Tree"
      ],
      "metadata": {
        "id": "MEcH-yyb8vjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we have the decision tree model which utilizes iteration through different features to make boundaries within the features that will lead to branching paths that once more have boundaries on a different or even the same feature checked. Decision trees do not work very well with featuresets that have many different unique values per feature. Thus, our decision tree models will likely perform worse than either of our previous models."
      ],
      "metadata": {
        "id": "j3zz6eyG5tSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
        "\n",
        "dt_classifier6 = nltk.DecisionTreeClassifier.train(train_set6)\n",
        "dt_classifier3 = nltk.DecisionTreeClassifier.train(train_set3)\n",
        "dt_classifier5 = nltk.DecisionTreeClassifier.train(train_set5)\n",
        "dt_classifier2 = nltk.DecisionTreeClassifier.train(train_set2)\n",
        "\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our cut down suffix and prefix classifier is {nltk.classify.accuracy(dt_classifier6, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix and prefix classifier is {nltk.classify.accuracy(dt_classifier5, devtest_set)*100}%\n",
        "The gender prediction accuracy for our large featureset classifier is {nltk.classify.accuracy(dt_classifier2, devtest_set)*100}%\n",
        "The gender prediction accuracy for our suffix based classifier is {nltk.classify.accuracy(dt_classifier3, devtest_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez4WD1W0q1Wa",
        "outputId": "d0ed38d9-6061-4da6-f9d3-5a08c81201fe"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our cut down suffix and prefix classifier is 73.6%\n",
            "The gender prediction accuracy for our suffix and prefix classifier is 73.6%\n",
            "The gender prediction accuracy for our large featureset classifier is 80.0%\n",
            "The gender prediction accuracy for our suffix based classifier is 76.6%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, our decision tree models were a downgrade in accuracy for almost every featureset. It's only our large featureset classifier which contains multiple binary features that retains its accuracy of the naive-bayes. Even then, a maximum entropy classifier version of that featureset would be better to use.\n",
        "\n",
        "In this case our data is not suited to the model and we are left with the naive-bayes cut down suffix and prefix classifier as the leading candidate for the final model."
      ],
      "metadata": {
        "id": "FXZsPGDE7cmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Conclusion"
      ],
      "metadata": {
        "id": "7R3_PRKS9ASD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we test the final versions of each classifier against the test set to see which model is possibly the best for predicting our data."
      ],
      "metadata": {
        "id": "d4rtRpXHESeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
        "\n",
        "print(f\"\"\"\n",
        "The gender prediction accuracy for our cut down naive-bayes suffix and prefix classifier is {nltk.classify.accuracy(classifier6, test_set)*100}%\n",
        "The gender prediction accuracy for our maximum entropy large featureset classifier is {nltk.classify.accuracy(maxent_classifier2, test_set)*100}%\n",
        "The gender prediction accuracy for our decision tree large featureset classifier is {nltk.classify.accuracy(dt_classifier2, test_set)*100}%\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eur-23Ra8NOy",
        "outputId": "a2499197-bacd-42b1-bd7a-371b1cd0415c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The gender prediction accuracy for our cut down naive-bayes suffix and prefix classifier is 80.60000000000001%\n",
            "The gender prediction accuracy for our maximum entropy large featureset classifier is 80.0%\n",
            "The gender prediction accuracy for our decision tree large featureset classifier is 78.60000000000001%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the best of each type of model evaluated against the test set we see that in a measure of pure accuracy our cut-down naive-bayes suffix and prefix classifier narrowly beats out the maximum entropy large featureset model. The decision tree classifier isn't even in the competition here. There is a slight decrease from our dev-test accuracy, but that's to be expected for any model and it is not a concerning amount.\n",
        "\n",
        "Naive-bayes models are at their best with distinct non-overlapping features that provide important information towards classifying, which ends up being the best way to classify names since they are typically short and low on data to extract from that would make a maximum entropy model stand out. For a decision tree model to work here we would need names to have features that had only a few possible values to contribute information to gender."
      ],
      "metadata": {
        "id": "UrVkqY94894p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics import ConfusionMatrix\n",
        "\n",
        "test_class = classifier6.classify_many([x for x,y in test_set])\n",
        "test_true = [y for x,y in test_set]\n",
        "\n",
        "cm = ConfusionMatrix(test_true,test_class)\n",
        "\n",
        "print(cm)\n",
        "print(cm.evaluate())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mc7S9dNCBLE",
        "outputId": "e46f309c-7294-4302-aa1b-926b67c1e760"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       |   f     |\n",
            "       |   e     |\n",
            "       |   m   m |\n",
            "       |   a   a |\n",
            "       |   l   l |\n",
            "       |   e   e |\n",
            "-------+---------+\n",
            "female |<288> 29 |\n",
            "  male |  68<115>|\n",
            "-------+---------+\n",
            "(row = reference; col = test)\n",
            "\n",
            "   Tag | Prec.  | Recall | F-measure\n",
            "-------+--------+--------+-----------\n",
            "female | 0.8090 | 0.9085 | 0.8559\n",
            "  male | 0.7986 | 0.6284 | 0.7034\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the confusion matrix we see that our final naive-bayes model ends up classifying females more correctly than males while it is only accurate 2/3 of the time for males. This is useful info to know since our data had more female names than males it is possible that our model is relying a bit on that to classify names more as female rather than the characteristics."
      ],
      "metadata": {
        "id": "vhZLlPstGB_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Video Presentation"
      ],
      "metadata": {
        "id": "-pgx8Yxu9MxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below allows a YouTube link to the video presentation to be inserted for the url variable and will then display the YouTube video within the notebook itself.\n",
        "\n",
        "A regex match extracts the video ID from the URL which is then fed into the IPython package's built in Youtube embedder."
      ],
      "metadata": {
        "id": "ICW8tUu15X3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://youtu.be/tYksib7BFWA\""
      ],
      "metadata": {
        "id": "uGhP5XMT7yHL"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "import re\n",
        "\n",
        "reg = r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\"\n",
        "urlid = re.search(reg, url)[1]\n",
        "\n",
        "YouTubeVideo(urlid, width=800, height=450)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "gp6ithnRIjDA",
        "outputId": "507434cc-3c86-4854-c930-db5acf491c1c"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7f1a0cc8c5e0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"450\"\n",
              "            src=\"https://www.youtube.com/embed/tYksib7BFWA\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    }
  ]
}